{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mh5Biak3wi1a"
   },
   "source": [
    "## PHYS-467 Machine Learning for Physicists. Exercise session 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "km9iVWGVW8i4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle, os\n",
    "from urllib.request import urlopen \n",
    "\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression, Ridge, Lasso, RidgeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vu0nApvCxGOO"
   },
   "source": [
    "# Exercise 1: Linear Regression (clarifications)\n",
    "\n",
    "In this first exercise, we'll clarify the link between Gradient Descent, the pseudo-inverse and regularization for Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.a)** Generate a random $n\\times{d}$ data matrix $X$ where $n=5$ and $d=10$. Generate an $n-$ dimensional _random_ label vector $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.b)** Use sklearn's LinearRegression to solve the OLS problem on the previously generated data. Use the option fit_intercept=False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.c)** Recall that the OLS solution is given by $w = (X^TX)^{-1}X^Ty$. Note that when $n<d$, the matrix $X^TX$ is not invertible and we should resort to the pseudoinverse of $X$. In this case the solution reads $w = X^T(XX^T)^{-1}y$. Write a function that returns the solution calculated in this way and compare the so-obtained coefficients with those obtained by sklearn. What can you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4WU0fBC6xYuU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.c)** Another way to make the matrix invertible is to do Ridge Regression. Implement a function returning the regularized solution and compare with the solution obtained by sklearn's Ridge, use $\\lambda=1$ for both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.d)** Use a small regularization parameter, say $\\lambda = 0.001$ and compare the output of Ridge regression with the pseudo-inverse. What do you notice ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.e)** Compute the square error on the training data, obtained by using the pseudoinverse coefficient and the error obtained with the regularised coefficient (with $\\lambda = 0.1$). Which one do you expect to be smaller?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.f)** Consider the OLS problem and implement a function performing one step of gradient descent w.r.t. the parameters. Implement a second function calling the first function for M iterations and finally returning the optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.g)** Compute 1) $w_a$ the parameters obtained by initializing gradient descent with a vector of zeroes, and $w_b$ the parameters obtained by initializing gradient descent with a vector of ones. Compare both $w_a$ and $w_b$ with the solution obtained by the pseudo-inverse. What do you notice ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.h)** Generate a random data $n\\times{d}$ matrix $X$ where $n=10$ and $d=5$. Generate an $n-$dimensional label vector $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.i)** Use sklearn's LinearRegression to solve the OLS problem on the previously generated data. Use the option fit_intercept=False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.l)** Write a function returning the standard OLS solution and compare it with the sklearn's solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.m)** Compute the training error obtained by using the OLS coefficient and the error obtained with the regularised coefficient (with $\\lambda = 0.1$). Which one do you expect to be smaller?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.n)** Use the GD function implemented before and compare the solution it finds with the one found by sklearn's LinearRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 : (Stochastic) Gradient Descent with Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.a)** Implement the sigmoid function $\\sigma(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.b)** Generate an $n\\times{d}$ random data matrix with dimension $d = 2$. Generate a random vector of optimal parameters $w_{opt}$. Generate the labels as $y = \\pm 1$ from the distribution $p(y = 1 | x) = 1 - p(y = -1 | x) = \\sigma(w_{opt}^Tx)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.d)** Implement a function performing one step gradient descent for logistic regression. Recall that with $y = \\pm 1$,  the logistic loss is $\\mathcal L = - \\sum_i \\log \\sigma(y_i \\times w^T x_i)$. Implement a second function enabling multiple iterations of gradient descent. This function should return the parameters calculated at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.e)** Implement stochastic gradient descent using the gradient step function used in the previous question. Hint: you should apply the gradient step function only on a random subset of the data at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.f)** Run GD and SGD with a small learning rate $\\eta$ ($10^{-2}$ should work fine). Plot the cosine $\\frac{w_t \\cdot w_{\\star}}{\\vert{w_t}\\vert\\vert{w_{\\star}\\vert}}$ as a function of the number of iterations $t$ for Gradient Descent and Stochastic Gradient Descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.g)** Use sklearn's LogisticRegression with fit_intercept=False and compare its accuracy with that obtained by gd and the optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.h** In this question we look at the role of the learning rate in the convergence of gradient descent. Run GD with a large learning rate (say $\\eta$ = 10.0), and observe that GD does not converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.a)** Generate a $n\\times{d}$ matrix with entries drawn from a Gaussian with zero mean and unit variance. Choose $n=300$ and $d=10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.b)** Given the vector $w_{opt}$ provided below, print it and use it to generate the training and testing labels. Corrupt the labels with zero-mean unit-variance Gaussian noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m w_opt \u001b[38;5;241m=\u001b[39m rng\u001b[38;5;241m.\u001b[39mchoice([\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\u001b[43md\u001b[49m,p\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.7\u001b[39m,\u001b[38;5;241m0.15\u001b[39m,\u001b[38;5;241m0.15\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'd' is not defined"
     ]
    }
   ],
   "source": [
    "w_opt = np.random.choice([0,1,-1], d ,p=[0.7,0.15,0.15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.c)** Perform linear regression on the training data and print: a) the so-found coefficients and the train error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.d)** Fit sklearn's LASSO on the training data and print: a) the so-found coefficients and the train error. Fix the regularization to 0.1. Look at the coefficients, what do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.e)** Use cross validation to find the optimal value of the regularization strength in Lasso. Plot the different coefficients as a function of $\\lambda$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Ridge Classification, Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we first consider a synthetic dataset to compare Ridge and Logistic regression for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.a)** Use the sklearn function make_classification to generate 1000 data samples in 2 dimension. Use the following arguments: n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=12. Then use train_test_split with the following arguments X, y, test_size=0.2, random_state=42, to generate training and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.b)** Make a scatter plot of the training data. Color each point according to its cluster assignment. _Hint_ : To change the color as a function of the label, use the option `c=y` in `plt.scatter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.c)** Train a ridge classifier and a logistic regression using sklearn. Compare the results in terms of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.d)** Plot the decision boundaries of the two models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundaries(X_test,y_test,ridge_classifier, logistic_regression, accuracy_ridge,accuracy_logistic):\n",
    "    \"\"\"\n",
    "    arguments : \n",
    "        - X_test, y_test : test data\n",
    "        - ridge_classifier : instance of the class RidgeClassifier\n",
    "        - logistic_regression : instance of the class LogisticRegression\n",
    "        - accuracy_ridge : accuracy of the ridge classifier\n",
    "        - accuracy_logistic : accuracy of the logistic regression\n",
    "    \"\"\"\n",
    "    # Plot the decision boundary for Ridge Classifier\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='coolwarm')\n",
    "    plt.title(f\"Ridge Classifier\\nAccuracy: {accuracy_ridge:.2f}\")\n",
    "    ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 50), np.linspace(ylim[0], ylim[1], 50))\n",
    "    Z = ridge_classifier.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='k')\n",
    "    \n",
    "    # Plot the decision boundary for Logistic Regression\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='coolwarm')\n",
    "    plt.title(f\"Logistic Regression\\nAccuracy: {accuracy_logistic:.2f}\")\n",
    "    ax = plt.gca()\n",
    "    Z = logistic_regression.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap='coolwarm', alpha=0.8)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.e)** Implement ridge classification from scratch. Implement two functions: one should return the weights and biases, the other should return the classification results given the inputs and the previously obtained coefficients. Add the option to include a regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.f)** Plot the decision boundary and compare it with that obtained via sklearn. Use alpha = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5: Classification on the Ising model \n",
    "\n",
    "We will now apply regression to the Ising model to predict the phase of the system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ising model parameters\n",
    "\n",
    "L=40 # linear system size\n",
    "J=-1.0 # Ising interaction\n",
    "#T=np.linspace(0.25,4.0,16) # set of temperatures\n",
    "T_c=2.26 # Onsager critical temperature in the TD limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.a)**\n",
    "\n",
    "1) Load the data using the provided function `load_data`, then split the data in three categories : the first **70000** samples are configurations in the **ordered** phase, the next **30000** samples are in the **critical** phase and the last **60000** samples are in the **disordered** phase.\n",
    "\n",
    "2) Merge the ordered and disordered samples together, use half of them for a training set and the rest for a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Loads the data of the Ising model. The labels correspond to ordered (1) and disordered states (0).\n",
    "    \"\"\"\n",
    "    # url to data\n",
    "    url_main = 'https://physics.bu.edu/~pankajm/ML-Review-Datasets/isingMC/';\n",
    "    ######### LOAD DATA\n",
    "    # The data consists of 16*10000 samples taken in T=np.arange(0.25,4.0001,0.25):\n",
    "    data_file_name = \"Ising2DFM_reSample_L40_T=All.pkl\" \n",
    "    # The labels are obtained from the following file:\n",
    "    label_file_name = \"Ising2DFM_reSample_L40_T=All_labels.pkl\"\n",
    "    #DATA\n",
    "    data = pickle.load(urlopen(url_main + data_file_name)) # pickle reads the file and returns the Python object (1D array, compressed bits)\n",
    "    data = np.unpackbits(data).reshape(-1, 1600) # Decompress array and reshape for convenience\n",
    "    data=data.astype('int')\n",
    "    data[np.where(data==0)]=-1 # map 0 state to -1 (Ising variable can take values +/-1)\n",
    "\n",
    "    #LABELS (convention is 1 for ordered states and 0 for disordered states)\n",
    "    labels = pickle.load(urlopen(url_main + label_file_name)) # pickle reads the file and returns the Python object (here just a 1D array with the binary labels)\n",
    "    return data, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.b)** Plot one instance of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set colourbar map\n",
    "#cmap_args=dict(cmap='plasma_r')\n",
    "\n",
    "# plot states\n",
    "#fig, axarr = plt.subplots(nrows=1, ncols=3)\n",
    "\n",
    "#axarr[0].imshow(X_ordered[20001].reshape(L,L),**cmap_args)\n",
    "#axarr[0].set_title('$\\\\mathrm{ordered\\\\ phase}$',fontsize=16)\n",
    "#axarr[0].tick_params(labelsize=16)\n",
    "\n",
    "#axarr[1].imshow(X_critical[1000].reshape(L,L),**cmap_args)\n",
    "#axarr[1].set_title('$\\\\mathrm{critical\\\\ region}$',fontsize=16)\n",
    "#axarr[1].tick_params(labelsize=16)\n",
    "\n",
    "#im=axarr[2].imshow(X_disordered[50000].reshape(L,L),**cmap_args)\n",
    "#axarr[2].set_title('$\\\\mathrm{disordered\\\\ phase}$',fontsize=16)\n",
    "#axarr[2].tick_params(labelsize=16)\n",
    "\n",
    "#fig.subplots_adjust(right=2.0)\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.c)** Train a Ridge and Logistic regression model using different regularization and evaluate the performance on the validation set and the samples in the critical phase. What do you notice ?\n",
    "_Note_ : Training the models might take some time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "exercise",
   "language": "python",
   "name": "exercise"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
