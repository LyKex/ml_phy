{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 12: Sequence-to-Sequence modelling using transformers\n",
        "\n",
        "In this exercise, we train a very small transformer architecture on an algorithmic toy task.\n",
        "\n",
        "We then 'introspect' the trained model (i.e. inspect its activations for different data samples), to find out what algorithm it learned and when it (does not) achieve the correct predictions."
      ],
      "metadata": {
        "id": "t5rSlUkE7LID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "from torch.utils.data import Dataset, random_split\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "d3Jra-EEY0z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Creating a sequence-to-sequence task\n",
        "\n",
        "In language modelling, the primary application of transformers, we often aim to map a sequence of words to a second sequence of words.\n",
        "Think about translating a sentence from one language to another, or ChatGPT creating an answer given a question by you.\n",
        "To make clear what the model sees when you translate a sentence, consider this example:\n",
        "$$\n",
        "[\\textrm{The, dog, chases, the, cat, .}] \\to [\\textrm{Der, Hund, jagt, die, Katze, .}]\n",
        "$$\n",
        "or\n",
        "$$\n",
        "[\\textrm{What, is, 2, times, 2, ?}] \\to [\\textrm{The, answer, is, 4, .}]\n",
        "$$\n",
        "\n",
        "In the machine learning context, a single word is called a \"token\". The set of all possible tokens is called \"alphabet\". Current open source models like BERT that can be downloaded on the internet have about ~30,000 tokens.\n",
        "\n",
        "In this exercise, we focus on a very specific sequence-to-sequence task, namely predicting the histogram of a sequence.\n",
        "Here, our tokens are just integer numbers from one to $T$, and an example is the following:\n",
        "$$\n",
        "[21,15,21,4,4,4,17] \\to [2,1,2,3,3,3,1]\n",
        "$$\n",
        "\n",
        "More formally,assume you are given the sequence of tokens $\\mathbf s = [s_1,s_2,\\cdots,s_d]$ of length $d$ (`seq_len` in the code) where $s_i \\in [1,\\cdots,T]$, we want to predict its histogram, the sequence $\\mathbf h^s = \\mathrm{hist}(\\mathbf s)$.\n",
        "At the position of the token $s_i$ in the original sequence, we find the number of times it occured in $S$, i.e.\n",
        "$$h^s_i = |\\{j =1,...,d| s_j = s_i \\}|.$$\n",
        "\n",
        "\n",
        "In the following we provide the dataset to load this data for a given length of the sequence $d$ (`seq_len`) and number of tokens $T$.\n"
      ],
      "metadata": {
        "id": "h8zaquOqqhCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hist(s):\n",
        "  c = Counter(s)\n",
        "  c = {w: c[w] for w in c}\n",
        "  return [c[w] for w in s]\n",
        "\n",
        "class HistogramDataset(Dataset):\n",
        "    def __init__(self, seq_len, T, n_samples,seed=42):\n",
        "        self.seq_len = seq_len\n",
        "        self.T = T\n",
        "        self.n_samples = n_samples\n",
        "        rs = np.random.RandomState(seed)\n",
        "        self.X = rs.randint(0, T, (n_samples, seq_len))\n",
        "        self.X = np.unique(self.X, axis=0)\n",
        "        self.y = np.empty_like(self.X)\n",
        "        for i in range(n_samples):\n",
        "          self.y[i] = hist(self.X[i])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.X[idx],dtype=torch.long), torch.tensor(self.y[idx],dtype=torch.long)"
      ],
      "metadata": {
        "id": "8DpMJXKw9EQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 1.1\n",
        "Generate a dataset for the histogram with a $T$ and $d$ of your choice. Print two samples from the dataset."
      ],
      "metadata": {
        "id": "bPpwlyl7lKR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "... # your code here"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKctI3dnlJXV",
        "outputId": "602e90e9-fb27-4efc-8368-dfd3c06822ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ellipsis"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question  1.2\n",
        "Given $T, d > 0$ and a sequence $\\mathbf s$ given that is generated from the data. What is the largest possible value that any $h^s_i$ can achieve? What is the smallest value?\n",
        "\n",
        "Verify that this is the case for an example from the dataset."
      ],
      "metadata": {
        "id": "BT7BhDrtTxax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "... # your code here"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKsK7uhRTjVw",
        "outputId": "584a603f-0c1c-4efb-94b3-f5cd6744a4c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ellipsis"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following code, we define the dataset and a train and validation split that we will then use to train and evaluate out transformer model that learns the $\\mathrm{hist}$ function."
      ],
      "metadata": {
        "id": "HwH7frv6mo6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the length, maximum value, and number of samples\n",
        "seq_len = 10\n",
        "T = 15\n",
        "num_samples = 10000\n",
        "n_classes = seq_len+1\n",
        "\n",
        "# Create the dataset\n",
        "dataset = HistogramDataset(seq_len, T, num_samples)\n",
        "\n",
        "# Split the dataset into train, test, and validation sets\n",
        "train_ratio, val_ratio = 0.7, 0.3\n",
        "\n",
        "train_size = int(train_ratio * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
      ],
      "metadata": {
        "id": "a6PByUfWax_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Building A Transformer - First layer: Word Embeddings"
      ],
      "metadata": {
        "id": "LKSm098c62Ow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following we define a \"mini\" transformer. The \"mini\" is to note that it does not have all the features as the transformer architecture when it was introduced, and to note that it would never be able to achieve SOTA results on common language tasks due to its limited capacity.\n",
        "\n",
        "There are many details and specialities to the architecture and training modalities of transformers in the wild that are beyond the scope of this course, and that have also changed rapidly over the last few of years.\n",
        "\n",
        "The focus in this exercise is on developing a deep understanding of the heart of the transformer architecture that has been a constant, the dot product attention layer that you also discussed in the lecture.\n",
        "\n",
        "We go through the architecture of the transformer step-by-step for an example data batch and then create the complete model."
      ],
      "metadata": {
        "id": "1xIXWNlBtDwB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 2.1\n",
        "Verify that the first batch of samples of input tensor x and the output tensor y have the correct shapes (what are the shapes you would expect)?"
      ],
      "metadata": {
        "id": "Hr7ZJnmRytPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# take the first batch of data from the training data\n",
        "B = 7 # batch size\n",
        "x, y =  next(iter(DataLoader(train_dataset, batch_size=B)))"
      ],
      "metadata": {
        "id": "svnvNLmGyEeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "... # your code here"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azRAcHqzyeWI",
        "outputId": "f1396633-a9fd-4639-eef8-07e691191a1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ellipsis"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 2.2\n",
        "\n",
        "In the sequence to sequence task, our input and our output is categorial (an element of a fixed set of choices). While you have seen categorial outputs in classification, we did not yet work with categorial inputs.\n",
        "\n",
        "So how can we input the token 'cat' to the transformer? The solution is to use one-hot encodings, i.e. vectors $v$ of length $T$. To represent the word 'cat' using a vector $v_{\\mathrm{cat}}$ we set the elements of the vector to zero except at position $i_{\\mathrm{cat}}$ where we set it to $1$.\n",
        "\n",
        "That means, the first layer of a model that processes such categorial input has to be of size $W_{\\mathrm{emb}} \\in \\mathbb{R}^{T\\times r}$, were $r$ is the model dimension for further processing (called `model_dim` in the code). When the remainder of the model architecture is added and the model is trained on a given task, the weights $W_{\\mathrm{emb}}$ are trained along with other parameters of the model.\n",
        "\n",
        "Often, one attributes a special name to this very first processing layer, it is called the `word embedding`. By multiplying $W_{\\mathrm{emb}}$ with a one-hot vector, one selects row of the matrix. Therefore, every row represents a single token, you can think of the row $[W_{\\mathrm{emb}}]_{i_{\\mathrm{cat}}}$ as a representation of cat in a $r$-dimensional space.\n",
        "\n",
        "In torch, the process of one-hot encoding and multiplying by the embedding matrix is already implemented in the `nn.Embedding` module, which we now use to create the embedding for our histogram task.\n",
        "\n",
        "- Write out in latex, what the one-hot encoding for the token '3' would look like, if overall there are 10 different tokens in my alphabet.\n"
      ],
      "metadata": {
        "id": "LtQVl6KMqsZC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your answer here:**"
      ],
      "metadata": {
        "id": "x3vmCClHXhoE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 2.2\n",
        "\n",
        "Have a look at the documentation of the `nn.Embedding` module in torch.\n",
        "You can use it in a similar fashion you used the torch sequential model.\n",
        "You first create an instance, e.g. `model = nn.Sequential(...,...)`. Afterwards you apply the module to some input by calling `output = model(input)`.\n",
        "\n",
        "Create an embedding module from `nn.Embedding`. So far there is no need to train the parameters within, we will just look at what it does.\n",
        "\n",
        "Apply the randomly initialized embedding to the batch `x` of size `B x seq_len` and create an embedding `e` of size `B x seq_len x model_dim`.\n",
        "\n",
        "Verify by example that the same token at different positions in the input `x` is mapped to to the same embedding vector.\n",
        "\n"
      ],
      "metadata": {
        "id": "SlbchJe70dEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_dim = 32\n",
        "... # your code here"
      ],
      "metadata": {
        "id": "O6LBEMn8zZ83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Building a Transformer: The Dot-Product Attention Layer\n",
        "\n",
        "The dot product attention is defined as follows for the $Q, K, V \\in \\mathbb R^{r \\times r}$ and $X \\in \\mathbb R^{d \\times r}$, so that a row in the matrix is the embedded token $\\mathbf x_i = [x_{i1},x_{i2},\\cdots,x_{ir}] \\in \\mathbb R^r$.\n",
        "The outcome of the attention layer at a specific token is then the linear combination of all other tokens, transformed acccording to the value matrix $V$.\n",
        "\n",
        "$$\n",
        "\\text{AttentionLayer}(X; Q, K, V)_{i} =  A_{i}V \\mathbf X \\in \\mathbb R^r\n",
        "$$\n",
        "\n",
        "where $A_{i}$ is an $d$-dimensional vector respresenting a probability distribution over all other tokens. The value $a_{ij}$ represents how much attention is payed from token at position $i$ to position $j$.\n",
        "This attention value is  computed as a query type funtion:\n",
        "\n",
        "$$\n",
        "A_i = \\text{att_probs}(X; Q, K)_{i} = [a_{i1},a_{i2},\\cdots,a_{id}] = \\text{softmax}\\left(\\frac{(\\mathbf x_iQ) (XK)^T}{\\sqrt{r}}\\right) \\in \\mathbb R^d\n",
        "$$\n",
        "\n",
        "We transform the embedding $\\mathbf x_i$ into a query of dimension $r$ via the matrix $Q$, all the other tokens including itself get transformed to keys using $K$. The comparison between the $d$ keys and the query from token $i$ is via the dot-product (think cosine similarity, so a high value when the vectors representing keys and queries point in the same direction ~represent similar concepts).\n",
        "Finally, the softmax transforms this vector of cosine similarities of $d$ values into a probability distribution, so that every value is between 0 and 1 and they all together sum to 1."
      ],
      "metadata": {
        "id": "A_J5fd7lqjgY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 3.1\n",
        "\n",
        "Below, we give you a skeleton for a cusom nn.Module, that we will later use in a sequential module to built a complete transformer architecture.\n",
        "\n",
        "Implement the attention mechanism in the forward function.\n",
        "\n",
        "Hint: Do this together with question 3.2, so your debugging is easier."
      ],
      "metadata": {
        "id": "IPdfYYdHXzEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DotProductAttention(nn.Module):\n",
        "  def __init__(self, d):\n",
        "    super(DotProductAttention, self).__init__()\n",
        "\n",
        "    self.model_dim = model_dim\n",
        "\n",
        "    self.Q = nn.Parameter(torch.empty(model_dim, model_dim))\n",
        "    self.K = nn.Parameter(torch.empty(model_dim, model_dim))\n",
        "    self.V = nn.Parameter(torch.empty(model_dim, model_dim))\n",
        "\n",
        "    nn.init.kaiming_uniform_(self.Q.T, a=math.sqrt(5))\n",
        "    nn.init.kaiming_uniform_(self.K.T, a=math.sqrt(5))\n",
        "    nn.init.kaiming_uniform_(self.V.T, a=math.sqrt(5))\n",
        "\n",
        "    self.attn_probs = None # we want to readout the attention matrix later\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x: B x seq_len x model_dim\n",
        "    Q = self.Q # model_dim x model_dim\n",
        "    K = self.K # model_dim x model_dim\n",
        "    V = self.V # model_dim x model_dim\n",
        "\n",
        "    # precompute important quantities\n",
        "    Qx =  ... # B x seq_len x model_dim\n",
        "    Kx =  ... # B x seq_len x model_dim\n",
        "    Vx =  ... # B x seq_len x model_dim\n",
        "\n",
        "    # compute the attention scores\n",
        "    attn_scores = ... # B x seq_len x seq_len\n",
        "\n",
        "    # apply the softmax\n",
        "    attn_probs = ...  # B x seq_len x seq_len\n",
        "    self.attn_probs = attn_probs # save attention matrix for later\n",
        "\n",
        "    # do a linear combination of the values\n",
        "    x = ... # B x seq_len x model_dim\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "bKyLFxqcVkqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 3.2\n",
        "\n",
        "Apply and test the dot product to the attention you coded to the embeddings `e` you created previously.\n",
        "\n",
        "Hint: Remember that it is used like any other torch module used before, you first instantiate it and then you apply it to a vector by calling it on some input."
      ],
      "metadata": {
        "id": "0yVDjjV_f0nN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "... # your code here"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6rh-jr6MXqD",
        "outputId": "83524c4b-0081-45d7-b6f3-3b29be8b16d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([7, 10, 32]), torch.Size([7, 10, 32]), torch.Size([7, 10, 10]))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Building a Transformer: Final Architecture\n",
        "\n",
        "We now successfully mixed the tokens in a given sequence, by mixing we mean: at the position of token $i$ there is now a linear combination of all other tokens in the sequence.\n",
        "\n",
        "Now to fit a function like the histogram task, we still have to process the information that got collected in an intelligent way.\n",
        "\n",
        "To do so, we apply an MLP module to the outcomes of the attention layer: A one hidden layer MLP with $h$ hidden neurons and relu activation. Since we want to predict tokens from our target alphabet, the output should correspond to the possible number of classes (in our case they range from 0 to L for the histogram task).\n",
        "We can use the `nn.Sequential` module to combine all necessary steps.\n",
        "\n",
        "#### Question 4.1\n",
        "Complete the model below and verify that it successfully tranforms the input batch $x$. What is the shape of the outcome? Does it make sense?"
      ],
      "metadata": {
        "id": "xjIaHnYtgfAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p = 128\n",
        "n_classes = seq_len+1\n",
        "transformer = nn.Sequential(nn.Embedding(T, model_dim),\n",
        "                            DotProductAttention(model_dim),\n",
        "                            nn.LayerNorm(model_dim), # we add layernorm to make training faster, but you can ignore what it does (but if you remove it you might have to wait a bit until training converges)\n",
        "                            ... ) # TODO: add a fully connected network with h hidden neurons and relu activations\n",
        "\n",
        "output = transformer(x)\n",
        "output.shape"
      ],
      "metadata": {
        "id": "8jsCRbdLhR_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 4.2\n",
        "How do you extract a prediction for the output sequence from output?"
      ],
      "metadata": {
        "id": "giOyoS4xidTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "... # your code here"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pPS7lE2iiKB",
        "outputId": "9d6fa2dc-a893-4f77-e7be-0d1384478191"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ellipsis"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 5: Training the Transformer\n",
        "\n",
        "You can have a look at how the training works, but it is not the objective of this exercise. Essentially we do supervised learning on the output sequence using the cross-entropy loss you know from classification with neural networks.\n",
        "\n",
        "Overall, executing the training should not last longer then 4min.\n"
      ],
      "metadata": {
        "id": "E8NqBZ3GhRWU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 5.1\n",
        "\n",
        "Paste the code for you transformer architecture from 4.1 and then run the training loop."
      ],
      "metadata": {
        "id": "CRIWng8Agejv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.001\n",
        "n_epochs = 100\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "transformer = nn.Sequential(...) # TODO: paste your solution from 4.2\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(transformer.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_acc = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for X, y in train_dataloader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = transformer(X)\n",
        "            loss = criterion(output.contiguous().view(-1, n_classes), y.contiguous().view(-1))\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        epoch_loss /= len(train_dataloader)\n",
        "        train_losses.append(epoch_loss)\n",
        "\n",
        "        # Evaluate on the test set every epoch\n",
        "        with torch.no_grad():\n",
        "            val_loss = 0.0\n",
        "            acc = 0.0\n",
        "            for X, y in val_dataloader:\n",
        "                output = transformer(X)\n",
        "                pred = output.argmax(axis=-1)\n",
        "                loss = criterion(output.view(-1,n_classes), y.view(-1))\n",
        "                acc += torch.mean((pred.view(-1)==y.view(-1)).float()).item()\n",
        "                val_loss+=loss.item()\n",
        "            val_loss /= len(val_dataloader)\n",
        "            acc /= len(val_dataloader)\n",
        "\n",
        "            val_losses.append(val_loss)\n",
        "            val_acc.append(acc)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "          print(f'[Epoch {epoch:02}] Train loss = {epoch_loss:.5f} :: Val loss {val_loss:.5f} :: Val accuracy {acc*100:.2f}')"
      ],
      "metadata": {
        "id": "j-RYb2ei874K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hint:** At this point, you should be able to obtain a validation accuracy around 97-99 %. If not, it might help to just restart the training another time."
      ],
      "metadata": {
        "id": "hqH69kD4Gl2y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 5.2\n",
        "\n",
        "Check the loss curves below, and the validation accuracy, does the transformer successfully fit the histogram task?"
      ],
      "metadata": {
        "id": "58xXwBp5jToW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses,label='test')\n",
        "plt.plot(val_losses,label='val')\n",
        "plt.xlabel('epochs')\n",
        "plt.legend(title='loss');"
      ],
      "metadata": {
        "id": "dHXhj3eo8l7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 6: Model Introspection\n",
        "In the final section of this exercise, we want to understand the algorithm the transformer learned, by probing predictions for specific corner-cases and looking at some of its internals.\n"
      ],
      "metadata": {
        "id": "8xV_TCHspoDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def highlight_cell(x,y,color):\n",
        "    # Given a coordinate (x,y), highlight the corresponding cell using a colored frame in the ac\n",
        "    # after having called imshow already\n",
        "    rect = plt.Rectangle((x-.5, y-.5), 1,1, fill=False,color=color,lw=2)\n",
        "    plt.gca().add_patch(rect)\n",
        "    return rect\n",
        "\n",
        "def visualize_attention_matrix(x):\n",
        "  x_ = torch.tensor(x,dtype=torch.long)\n",
        "  transformer(x_)\n",
        "  A = transformer[1].attn_probs\n",
        "  plt.imshow(A.detach().numpy(),vmin=0,cmap='winter')\n",
        "  for i, x_i in enumerate(x):\n",
        "    for j, x_j in enumerate(x):\n",
        "      if x_i == x_j:\n",
        "        highlight_cell(i,j, color='red')\n",
        "  plt.colorbar()\n",
        "  plt.xticks(np.arange(len(x)), x)\n",
        "  plt.yticks(np.arange(len(x)), x)\n",
        "  plt.xlabel('Input sequence')\n",
        "  plt.ylabel('Input sequence')\n",
        "  plt.title(f'Attention matrix')"
      ],
      "metadata": {
        "id": "fIDY1TdEktpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.1 What is the attention layer used for?\n",
        "Visualize the attention layer probabilities for some examples and highlight the cells that represent the attention onto tokens with the same token type using the given code.\n",
        "\n",
        "- Do you have a guess on what algorithm the model is learning?\n",
        "- Why is the attention layer useful?\n",
        "- Do you expect it to work for shorter/longer sequences?"
      ],
      "metadata": {
        "id": "quiufKrO6uZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = [1,1,3,2,3,3,0,0,1,1]\n",
        "visualize_attention_matrix(x)"
      ],
      "metadata": {
        "id": "4u166twpkgxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "... # test more examples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZumxuxZT_UmJ",
        "outputId": "38704c04-c6db-4000-b64d-8b92fd3975f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ellipsis"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.2 Does the transformer truly generalize?\n",
        "\n",
        "Since you now have a hypothesis of what your transformer learned, check its predictions carefully. Use the function we provide to predict a single sequence.\n",
        "\n",
        "- Is the validation error good?\n",
        "- Does it generalize to longer sequences?\n",
        "- Go back to the Histgram dataset and look at how the data is generated. What sequences are unlikely to be generated. Does it generalize those low probability input samples?\n",
        "- Does it generalize to unknown tokens?\n",
        "- Would you have expected the answers to the above questions, given the validation error you saw?"
      ],
      "metadata": {
        "id": "DfQwIcvr6rcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sequence(x):\n",
        "  pred = transformer.forward(torch.tensor(x,dtype=torch.long)).argmax(axis=-1).detach().numpy()\n",
        "  y = hist(x)\n",
        "  return list(pred), np.all(y == pred)"
      ],
      "metadata": {
        "id": "yBi3uF20_hzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_sequence([1,4,3,2,3,3,0,0,4])"
      ],
      "metadata": {
        "id": "NHhlrIfjASIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "... # try more examples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wex5yahMkhtj",
        "outputId": "84ecb4c5-2494-486c-9686-ba3d0aaf6b48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ellipsis"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus Part 7: What else?\n",
        "\n",
        "We kept this exercise rather concise, but of course with more interesting architectures there is more to explore. You also saw that even such an \"easy\" task as the histogram task is not trivially solved by a simple transformer.\n",
        "\n",
        "However, there are many ways to improve, tweaks to make, and architectural desicions to make.\n",
        "\n",
        "If you want a full example of a transformer architecture from scratch, we recommend the following online tutorial: [\"Transformer from scratch\"](https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb) that implements the [original transformer architecture](https://arxiv.org/pdf/1706.03762.pdf).\n",
        "\n",
        "Also feel free to think about the following, which are active or recent research areas in machine learning:\n",
        "- Where is the computational bottleneck for longer sentences? How can one circumvent it?\n",
        "- How does one make positional information enter the model to idenify positional information?\n",
        "- What are the training strategies when I want to predict the next word only, and not a complete sequence?\n",
        "\n",
        "Also note: It is not so difficult to use pretrained language models! Instead of training a complete model from scratch, if you want to use a model in application domains where there is true understanding of text, one can use pre-trained models available for free online (e.g. [hugging face](https://huggingface.co/)) and adapt them as you are doing in the current assignment ."
      ],
      "metadata": {
        "id": "zFriEYH4qSsG"
      }
    }
  ]
}