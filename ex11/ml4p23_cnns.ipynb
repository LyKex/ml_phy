{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kypHlqbaHSne"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1: Convolutional Neural Networks\n"
      ],
      "metadata": {
        "id": "6l20aHmi1mRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this exercise, you will train a simple CNN to classify images from the CIFAR10 dataset.\n"
      ],
      "metadata": {
        "id": "ien2qVja1jcc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Download the CIFAR10 dataset using `torchvision.datasets.CIFAR10`, and build the train and test dataloaders, setting the batch size to 32 and activating reshuffling at each epoch for the train data by setting `shuffle=True`. Visualize some images and their different color channels."
      ],
      "metadata": {
        "id": "PoCesiO81wz3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIYdn1woOS1n"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "training_data = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "test_data = ### YOUR CODE ###\n",
        "\n",
        "train_dataloader = ### YOUR CODE ###\n",
        "test_dataloader = ### YOUR CODE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5 # Unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "dataiter = iter(train_dataloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "print([classes[labels[j]] for j in range(batch_size)])"
      ],
      "metadata": {
        "id": "lKpRqs5sHgrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images.shape"
      ],
      "metadata": {
        "id": "QaK4YsoG8FVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imshow(images[0])"
      ],
      "metadata": {
        "id": "nKIbi8c48Iq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "im_red = images[0].clone()\n",
        "im_red[1:3,:,:] = 0.\n",
        "imshow(im_red) # Red channel of the image"
      ],
      "metadata": {
        "id": "BNheVsfh8dv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Define a function returning a convolutional neural network built with `nn.Sequential`. Use a first layer of 6 convolutional channels with filter size 5, a max-pooling layer over a $2 \\times 2$ window, a second convolutional layer made of 16 channels with filter size 5, another $2 \\times 2$ max-pooling layer, two dense layers with 120 and 84 neurons respectively, and a final linear layer with 10 outputs."
      ],
      "metadata": {
        "id": "UBAoNv-Y2eKj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE6KlzPwHcyi"
      },
      "source": [
        "def initialize_cnn():\n",
        "\n",
        "    ### YOUR CODE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Using the cross-entropy loss and SGD with learning rate 0.01, train the model for 5 epochs. After training, compute the accuracy on the test set."
      ],
      "metadata": {
        "id": "PJ1iB7kt3eug"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MC_ZFROYHLWX"
      },
      "source": [
        "model = initialize_cnn()\n",
        "\n",
        "### YOUR CODE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2: Momentum"
      ],
      "metadata": {
        "id": "phI7ce6j4n4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Making an analogy with a physical system, we can think of the negative gradient as a force moving a particle through parameter space, following Newtonâ€™s laws. Adding a momentum or inertia term, the optimization algorithm remembers the directions of the past gradients and continues to move in their direction. Mathematically,\n",
        "\\begin{equation}\n",
        "    v_t = \\gamma v_{t-1} + \\eta \\nabla_\\theta L(\\theta_t)\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "    \\theta_{t+1} = \\theta_t - v_t,\n",
        "\\end{equation}\n",
        "where $\\gamma \\in [0,1]$ is the momentum parameter, $\\eta$ the learning rate, and $\\theta$ the parameters of the model. Momentum helps the optimization dynamics gain speed in directions with persistent small gradients and suppresses oscillations. Repeat training, adding `momentum=0.9` to the SGD dynamics."
      ],
      "metadata": {
        "id": "D0OjKEia4gz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = initialize_cnn()\n",
        "\n",
        "### YOUR CODE ###"
      ],
      "metadata": {
        "id": "APJGy65T9DYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3: Feature Maps"
      ],
      "metadata": {
        "id": "n-3UVCvo4-Gg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Using t`orch.fx`, we can visualize the transformations of an input inside our neural network. For different input images, check the outputs of the first convolutional layer, of the first ReLU application, and of the first pooling layer.\n"
      ],
      "metadata": {
        "id": "rpcdgFjs5A9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.feature_extraction import get_graph_node_names\n",
        "from torchvision.models.feature_extraction import create_feature_extractor\n",
        "\n",
        "nodes, _ = get_graph_node_names(model)\n",
        "print(nodes) # Prints the nn.Sequential layer names\n",
        "\n",
        "feature_extractor = create_feature_extractor(\n",
        "\tmodel, return_nodes=['0', '1', '2']) # Outputs of first conv. layer, ReLU, and first pooling layer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaSpAT1kjISg",
        "outputId": "77596664-1cc0-4d24-a5af-11854487f6b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['input', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "\n",
        "image = ### YOUR CODE (Select one CIFAR Image) ###\n",
        "\n",
        "out = feature_extractor(image.unsqueeze(0)) # Return dictionary with the feature maps of 'image'\n",
        "\n",
        "imshow(image)\n",
        "\n",
        "print('conv1')\n",
        "for i in range(6):\n",
        "    plt.imshow(out['0'][0,i].detach())\n",
        "    plt.show()\n",
        "\n",
        "print('relu')\n",
        "for i in range(6):\n",
        "    plt.imshow(out['1'][0,i].detach())\n",
        "    plt.show()\n",
        "\n",
        "print('pool')\n",
        "for i in range(6):\n",
        "    plt.imshow(out['2'][0,i].detach())\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "2g-FdunN_Ar0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Can we look at what a particular neuron reacts to? What are the features learned by deep models? A simple idea to visualize these features, called activation maximization, consists in looking for the input with bounded norm that maximizes the activation of a given neuron ($x^* = \\arg \\max_{x: \\; \\|x\\|=1} h_i^{\\ell}(x,\\theta^*)$, where $h_i^\\ell$ is the activation of the neuron $i$ at layer $\\ell$ of a trained network). Open https://distill.pub/2017/feature-visualization/appendix and check how the neurons in different layers of the GoogLeNet network are specializing to recognize features with various complexity, from simple textures to meaningful semantic concepts!"
      ],
      "metadata": {
        "id": "5zS5x-uz5MiI"
      }
    }
  ]
}